cluster:
  kops:
    stateFile: s3://k8s-kops-csi-e2e
    zones: us-west-2a
    nodeCount: 3
    nodeSize: c5.large
    kubernetesVersion: 1.15.3
    iamPolicies: |2
        additionalPolicies:
          node: |
            [
              {
                "Effect": "Allow",
                "Action": [
                  "iam:CreateServiceLinkedRole",
                  "iam:AttachRolePolicy",
                  "iam:PutRolePolicy"
                ],
                "Resource": "arn:aws:iam::*:role/aws-service-role/s3.data-source.lustre.fsx.amazonaws.com/*"
              },
              {
                "Effect": "Allow",
                "Action": [
                  "s3:ListBucket",
                  "fsx:CreateFileSystem",
                  "fsx:DeleteFileSystem",
                  "fsx:DescribeFileSystems"
                ],
                "Resource": ["*"]
              }
            ]

build: |
  eval $(aws ecr get-login --region us-west-2 --no-include-email)
  AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  IMAGE_TAG={{TEST_ID}}
  IMAGE_NAME=$AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/aws-fsx-csi-driver
  docker build -t $IMAGE_NAME:$IMAGE_TAG .
  docker push $IMAGE_NAME:$IMAGE_TAG

install: |
  echo "Deploying driver"
  AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  IMAGE_TAG={{TEST_ID}}
  IMAGE_NAME=$AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/aws-fsx-csi-driver

  sed -i'' "s,chengpan/aws-fsx-csi-driver,$IMAGE_NAME," deploy/kubernetes/overlays/dev/kustomization.yaml
  sed -i'' "s,newTag: latest,newTag: \"$IMAGE_TAG\"," deploy/kubernetes/overlays/dev/kustomization.yaml
  kubectl apply -k deploy/kubernetes/overlays/dev/

uninstall: |
  echo "Removing driver"
  kubectl delete -k deploy/kubernetes/overlays/dev/

test: |
  go get github.com/onsi/ginkgo/ginkgo
  export KUBECONFIG=$HOME/.kube/config
  cluster_name=test-cluster-{{TEST_ID}}.k8s.local
  skip="\[Disruptive\]|should.provision.storage.with.mount.options|should.not.mount./.map.unused.volumes.in.a.pod|should unmount if pod is force deleted while kubelet is down"
  ginkgo -p -nodes=8 -skip=$skip -v ./tests/e2e -- --cluster-name=$cluster_name --region=us-west-2 --report-dir=$ARTIFACTS
